import os
import argparse
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from vector_store import VectorStore
from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, VECTOR_DB_PATH

import base64
from tqdm import tqdm
from rag_agent import RAGAgent # ç”¨äºè°ƒç”¨ Vision API


# ä½ çš„åŸºç¡€æ•°æ®è·¯å¾„
BASE_DATA_DIR = os.path.join(".", "data")

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def process_images_with_vision_model(chunks):
    """
    éå†æ–‡æ¡£å—ï¼Œæ‰¾åˆ°å›¾ç‰‡å—ï¼Œè°ƒç”¨è§†è§‰æ¨¡å‹ç”Ÿæˆæè¿°
    """
    agent = RAGAgent() # å®ä¾‹åŒ–ä»¥ä½¿ç”¨å…¶ä¸­çš„ vision_client
    processed_chunks = []
    
    print("\nğŸ‘ï¸ æ­£åœ¨è¿›è¡Œå›¾ç‰‡è¯­ä¹‰åˆ†æä¸æè¿°ç”Ÿæˆ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    
    image_chunks = [c for c in chunks if c.get("is_image")]
    text_chunks = [c for c in chunks if not c.get("is_image")]
    
    # å…ˆæŠŠçº¯æ–‡æœ¬æ”¾è¿›å»
    processed_chunks.extend(text_chunks)
    
    for chunk in tqdm(image_chunks, desc="åˆ†æå›¾ç‰‡", unit="å¼ "):
        try:
            img_path = chunk["image_path"]
            if not os.path.exists(img_path):
                continue
                
            base64_img = encode_image(img_path)
            
            # ä½¿ç”¨ Agent ä¸­å·²æœ‰çš„æ–¹æ³•ç”Ÿæˆæè¿°
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¤ç”¨ understand_imageï¼Œä½†æç¤ºè¯æ˜¯é’ˆå¯¹é€šç”¨æœç´¢ä¼˜åŒ–çš„
            description = agent.understand_image(base64_img)
            
            if description:
                # æ›´æ–°å†…å®¹ï¼šåŠ ä¸Šæ–‡ä»¶åä½œä¸ºå‰ç¼€ï¼Œå¢å¼ºæ£€ç´¢ç›¸å…³æ€§
                final_content = f"ã€å›¾ç‰‡å†…å®¹æè¿°ã€‘(æ–‡ä»¶: {chunk['filename']}, é¡µç : {chunk['page_number']})\n{description}"
                chunk["content"] = final_content
                # ç§»é™¤ is_image æ ‡è®°ï¼Œæˆ–è€…ä¿ç•™å®ƒç”¨äºåç»­é€»è¾‘ï¼Œè¿™é‡Œæˆ‘ä»¬è¦ä¿ç•™ image_path
                processed_chunks.append(chunk)
                
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡ {chunk.get('image_path')} å¤±è´¥: {e}")
    
    return processed_chunks









def main():
    # 1. è§£æå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--theme", type=str, default=None, help="æŒ‡å®šä¸»é¢˜æ–‡ä»¶å¤¹")
    parser.add_argument("--incremental", action="store_true", help="å¢é‡æ›´æ–°æ¨¡å¼")
    args = parser.parse_args()

    # 2. ç¡®å®šè·¯å¾„
    if args.theme:
        target_dir = os.path.join(BASE_DATA_DIR, args.theme)
    else:
        target_dir = BASE_DATA_DIR # é»˜è®¤å¤„ç†å…¨éƒ¨

    if not os.path.exists(target_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return

    print(f"ğŸ“‚ å¤„ç†ç›®å½•: {target_dir}")

    # 3. åˆå§‹åŒ–
    # æ³¨æ„ï¼šDocumentLoader ä¼šé€’å½’åŠ è½½ï¼Œæ‰€ä»¥å¦‚æœæ˜¯å¤„ç†å­æ–‡ä»¶å¤¹ï¼Œå®ƒåªä¼šåŠ è½½è¯¥æ–‡ä»¶å¤¹ä¸‹çš„
    loader = DocumentLoader(data_dir=target_dir)
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    vector_store = VectorStore(db_path=VECTOR_DB_PATH)
    

    # 4. æ¸…ç†ç­–ç•¥
    if not args.incremental:
        print("ğŸ§¹ å…¨é‡æ¨¡å¼ï¼šæ¸…ç©ºæ•°æ®åº“...")
        vector_store.clear_collection()
    else:
        print("â• å¢é‡æ¨¡å¼ï¼šä¿ç•™æ—§æ•°æ®...")

    # 5. æ‰§è¡Œå¤„ç†
    documents = loader.load_all_documents(specific_dir=target_dir)
    if not documents:
        print("âš ï¸ è¯¥ç›®å½•ä¸‹æ²¡æœ‰æ–‡æ¡£")
        return

    
# 6. åˆ‡åˆ†æ–‡æ¡£
    # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ä¿®æ”¹ TextSplitter ä»¥è·³è¿‡å·²ç»æ ‡è®°ä¸º is_image çš„å—ï¼Œæˆ–è€…åœ¨ split_documents åå¤„ç†
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ç®€å•çš„ç­–ç•¥ï¼šå…ˆåˆ‡åˆ†æ–‡æœ¬ï¼Œå›¾ç‰‡å—ä¿æŒåŸæ ·
    
    # ä¸´æ—¶ç­–ç•¥ï¼šæ‰‹åŠ¨åˆ†ç¦»
    raw_text_docs = [d for d in documents if not d.get("is_image")]
    raw_image_docs = [d for d in documents if d.get("is_image")]
    
    # åˆ‡åˆ†æ–‡æœ¬
    text_chunks = splitter.split_documents(raw_text_docs)
    
    # åˆå¹¶å›¾ç‰‡å—ï¼ˆæ— éœ€åˆ‡åˆ†ï¼Œå› ä¸ºæ¯ä¸ªå›¾ç‰‡å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„çŸ¥è¯†ç‚¹ï¼‰
    # å¹¶ä¸”è¦ç»™å›¾ç‰‡å—åŠ ä¸Šå¿…è¦çš„ chunk_id ç­‰å­—æ®µ
    image_chunks_formatted = []
    for i, img_doc in enumerate(raw_image_docs):
        img_doc["chunk_id"] = f"img_{i}"
        image_chunks_formatted.append(img_doc)
        
    all_chunks = text_chunks + image_chunks_formatted
    
    # 7. å…³é”®æ­¥éª¤ï¼šè°ƒç”¨è§†è§‰æ¨¡å‹å¢å¼ºæ•°æ®
    # åªæœ‰å½“å­˜åœ¨å›¾ç‰‡å—æ—¶æ‰è°ƒç”¨
    if image_chunks_formatted:
        all_chunks = process_images_with_vision_model(all_chunks)
    
    print(f"ğŸ’¾ å†™å…¥ {len(all_chunks)} æ¡æ•°æ® (å«æ–‡æœ¬ä¸å›¾ç‰‡æè¿°)...")
    
    # æ³¨æ„ï¼šç¡®ä¿ vector_store.add_documents èƒ½å¤„ç† metadata ä¸­çš„ None å€¼
    # æœ€å¥½åœ¨ add_documents å‰æŠŠ metadata æ¸…æ´—ä¸€ä¸‹ï¼ŒæŠŠ None è½¬ä¸ºç©ºå­—ç¬¦ä¸²
    for chunk in all_chunks:
        if "is_image" in chunk: del chunk["is_image"] # æ¸…ç†æ ‡è®°
        if chunk.get("image_path") is None: chunk["image_path"] = ""
            
    vector_store.add_documents(all_chunks)
    
    print("âœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()